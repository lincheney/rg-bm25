#!/usr/bin/env python3

import os
import sys
import json
import shutil
import sqlite3
import linecache
import itertools
import subprocess
from functools import partial, cache
import math
from collections import defaultdict

STDIN = '<stdin>'
FILENAME = 0
LINENO = 1
SIZE = 2
TEXT = 3
SUBMATCHES = 4

BAR = '▏'
COLOURS = dict(
    reset = '\x1b[0m',
    filename = '\x1b[0;95m',
    filename_match = '\x1b[0;1;4;35m',
    filename_header = '\x1b[48;5;235m',
    separator = '\x1b[0;37m',
    lineno = '\x1b[0;2;37m',
    lineno_match = '\x1b[0;37m',
    match = '\x1b[0;1;40;31m',
)

def get_file_size(filename):
    return os.stat(filename).st_size

def bm25(callback, n, k1=1.5, b=0.5, delta=1, dampen=0.75):
    tf = defaultdict(int)
    idf = defaultdict(set)
    doclen = defaultdict(int)
    callback(tf, idf, doclen)

    scores = defaultdict(int)
    avg_doclen = sum(doclen.values()) / (len(doclen) or 1)
    idf_score = {k: math.log1p((n - len(v) + 0.5) / (len(v) + 0.5)) for k, v in idf.items()}
    # idf_score = {k: math.log(n / len(v)) for k, v in idf.items()}
    for (filename, key), tf_score in tf.items():
        # tf_score = math.log1p(tf_score)
        # tf_score = tf_score ** dampen
        scores[filename] += idf_score[key] * ((tf_score * (k1 + 1)) / (tf_score + k1 * (1 - b + b * doclen[filename] / avg_doclen)) + delta)
    return scores

def bm25_file(data, tf, idf, doclen, filename_scale=1, word_boost=5):
    for x in data:
        filename = x[FILENAME]

        if x[SIZE] is not None and filename and filename != STDIN:
            doclen[filename] = len(filename) * filename_scale + x[SIZE]

        if x[TEXT]:
            scale = 1
            if filename == STDIN:
                filename = x[TEXT].rstrip('\n')
                scale = filename_scale
            encoded = x[TEXT].encode('utf8')
            for match, start, end in x[SUBMATCHES]:
                score = 1
                # boost matches at word boundaries
                if not encoded[start-1:start].isalnum():
                    score += word_boost
                if not encoded[end:end+1].isalnum():
                    score += word_boost
                score *= scale

                key = (x[FILENAME] == STDIN, match)
                tf[(filename, key)] += score
                idf[key].add(filename)

    for filename, key in tf:
        if filename not in doclen:
            doclen[filename] = len(filename) * filename_scale + get_file_size(filename)

def bm25_line(data, tf, idf, doclen):
    for k, x in data.items():
        if x[SUBMATCHES]:
            for match, start, end in x[SUBMATCHES]:
                tf[(k, match)] += 1
                idf[match].add(k)
            doclen[k] = len(x[TEXT])

def print_line(data, base, highlight):
    if isinstance(data, str):
        print(base, data.rstrip('\n'), COLOURS['reset'], sep='')
    else:
        line = data[TEXT].encode('utf-8')
        start = 0
        base = base.encode('utf8')
        highlight = highlight.encode('utf8')

        sys.stdout.buffer.flush()
        for m, s, e in data[SUBMATCHES]:
            sys.stdout.buffer.write(base + line[start:s] + highlight + line[s:e] + base)
            start = e
        sys.stdout.buffer.write(base + line[start:] + COLOURS['reset'].encode('utf8'))

def parse_ripgrep_json(lines):
    # if too much data, it can be faster to use sqlite to do json parsing
    if len(lines) > 10000 and sqlite3.sqlite_version_info >= (3, 38):
        db = sqlite3.connect(":memory:")
        db.execute('CREATE TABLE data (id INTEGER, json TEXT)')
        db.executemany('INSERT INTO data (id, json) VALUES (?, ?)', enumerate(lines))

        data = db.execute("""SELECT
            id,
            json_extract(json, '$.data.path.text'),
            json_extract(json, '$.data.line_number'),
            json_extract(json, '$.data.stats.bytes_searched'),
            json_extract(json, '$.data.lines.text')
        FROM data""")
        data = [x[1:]+([],) for x in sorted(data)]
        submatches = db.execute("""SELECT
            data.id,
            json_extract(value, '$.match.text'),
            json_extract(value, '$.start'),
            json_extract(value, '$.end')
        FROM data, json_each(data.json, '$.data.submatches') """)
        for id, match, start, end in submatches:
            data[id][SUBMATCHES].append((match.lower(), start, end))

        return data

    # this is faster than parsing each line separately
    data = (x['data'] for x in json.loads(b'[' + b','.join(lines) + b']'))
    data = [(
        x.get('path', {}).get('text'),
        x.get('line_number'),
        x.get('stats', {}).get('bytes_searched'),
        x.get('lines', {}).get('text'),
        [(y['match']['text'].lower(), y['start'], y['end']) for y in x.get('submatches', ())],
    ) for x in data]
    return data

def main():
    import argparse
    parser = argparse.ArgumentParser(epilog='Extra flags are passed to rg.')
    parser.add_argument('-d', '--directory', action='append', help='File or directory to search')
    parser.add_argument('-r', '--reverse', action='store_true', help='Show results in reverse order i.e. best matches at the top')
    parser.add_argument('-n', '--num-matches', type=int, metavar='N', default=20, help='Show only the best N matching files')
    parser.add_argument('-m', '--max-count', type=int, metavar='N', default=3, help='Show only N matching lines per file')
    parser.add_argument('-N', '--no-line-number', action='store_true', help='Suppress line numbers')
    parser.add_argument('-l', '--files-with-matches', action='store_true', help='Print only filenames and not file content')
    parser.add_argument('--colour', '--color', choices=('never', 'auto', 'always'), default='auto', help='Whether to show colours (default: %(default)s)')
    parser.add_argument('-C', '--context', type=int, metavar='N', default=1, help='Show N lines before and after matching lines')
    parser.add_argument('--no-delta', action='store_true', help='Do not use delta for syntax highlighting')
    parser.add_argument('-v', '--invert-match', action='store_true', help=argparse.SUPPRESS)
    parser.add_argument('pattern', nargs='*')
    args, extras = parser.parse_known_args()

    if args.invert_match:
        parser.error('-v/--invert-match is not supported')
    if not ''.join(args.pattern):
        parser.print_help()
        return
    args.directory = args.directory or ['.']

    istty = os.isatty(1)
    term_size = shutil.get_terminal_size()
    have_delta = not args.no_delta and shutil.which('delta')

    try:
        files = subprocess.check_output(['rg', '--files', *extras, *args.directory])
    except subprocess.CalledProcessError as e:
        return e.returncode

    command = ['rg', '-S', '--json', *extras]
    for t in args.pattern:
        command += ['-e', t]
    command += ['--', '-', *args.directory]

    data = subprocess.run(command, input=files, stdout=subprocess.PIPE).stdout.splitlines()
    data = parse_ripgrep_json(data)

    scores = bm25(partial(bm25_file, data), len(files.splitlines()))
    if not scores:
        return

    ranked = sorted(scores, key=scores.get, reverse=True)[:args.num_matches]
    if not args.reverse:
        ranked.reverse()

    filenames = {x[TEXT].rstrip('\n'): x for x in data if x[TEXT] and x[FILENAME] == STDIN}
    sizes = {x[FILENAME]: x[SIZE] for x in data if x[SIZE] is not None}
    data = (x for x in data if x[SUBMATCHES] and x[FILENAME] in ranked)
    data = itertools.groupby(data, lambda x: x[FILENAME])
    data = {k: {x[LINENO]: x for x in v} for k, v in data}

    if args.colour == 'never' or (args.colour == 'auto' and not istty):
        for k in COLOURS:
            COLOURS[k] = ''

    delta = None
    used_delta = False
    if not args.files_with_matches and istty and have_delta:
        delta = subprocess.Popen(
            ['delta', '--hunk-header-style=raw', '--grep-header-file-style=omit', '--tabs=0'],
            env={**os.environ, 'DELTA_FEATURES': '+rg-bm25 '+os.environ.get('DELTA_FEATURES', '')},
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            text=True,
        )

    for i, filename in enumerate(ranked, 1):
        if args.files_with_matches:
            print_line(filenames.get(filename, filename), COLOURS['filename'], COLOURS['filename_match'])
            continue
        print_line(filenames.get(filename, filename), COLOURS['filename'] + COLOURS['filename_header'], COLOURS['filename_match'] + COLOURS['filename_header'])

        scores = []
        if filename in data:
            # prioritise lines with many matches
            lines = linecache.getlines(filename)
            line_scores = bm25(partial(bm25_line, data[filename]), sizes.get(filename) or get_file_size(filename))
            matches = {k: set(m for m, s, e in v[SUBMATCHES]) for k, v in data[filename].items()}

            while len(scores) < args.max_count and any(matches.values()):
                best = max(matches.items(), key=lambda kv: (len(kv[1]), line_scores[kv[0]]))
                matches.pop(best[0])
                scores.append(best[0])
                for v in matches.values():
                    v.difference_update(best[1])
            scores.extend(sorted([x for x in data[filename] if x not in scores]))
            scores = sorted(scores[:args.max_count])

        output = set()
        for j in scores:
            lineno = data[filename][j][LINENO]
            output.update(range(max(1, lineno - args.context), min(len(lines), lineno + args.context)+1))
        output = [data[filename].setdefault(x, (filename, x, None, lines[x-1], (), (), ())) for x in sorted(output)]

        if output:
            if not delta:
                output = itertools.zip_longest(output, ())
            else:
                input = []
                input.append({"type": "begin", "data": {"path": {"text": filename}}})
                input.extend({
                    "type":"match",
                    "data":{
                        "path": {"text": filename},
                        "lines": {"text": ' \n' if x[TEXT] == '\n' else x[TEXT]},
                        "line_number": x[LINENO],
                        "absolute_offset": 0,
                        "submatches": [{'match': {'text': m}, 'start': s, 'end': e} for m, s, e in x[SUBMATCHES]],
                    },
                } for x in output)
                input.append({"type": "end", "data": {"path": {"text": filename}}})

                print('\n'.join(json.dumps(i) for i in input), file=delta.stdin, flush=True)
                if used_delta:
                    next(delta.stdout) # read the trailer
                next(delta.stdout) # read the filename
                used_delta = True
                output = zip(output, delta.stdout)

        lineno = 1
        for x, line in output:
            if lineno != 1 and x[LINENO] > lineno + 1:
                print(COLOURS['separator'], '---', COLOURS['reset'], sep='')
            lineno = x[LINENO]
            if not args.no_line_number:
                print(COLOURS['lineno_match'] if x[SUBMATCHES] else COLOURS['lineno'], '%-4s' % lineno, COLOURS['lineno'], BAR, COLOURS['reset'], sep='', end='')
            if line is None:
                print_line(x, COLOURS['reset'], COLOURS['match'])
            else:
                print(line, sep='', end='')

        if i != len(ranked):
            print(COLOURS['separator'], '─'*(term_size.columns - 4), COLOURS['reset'], sep='')

    if delta:
        delta.stdin.close()
        delta.wait()
    print(COLOURS['reset'], end='')

if __name__ == '__main__':
    try:
        sys.exit(main())
    except (KeyboardInterrupt, BrokenPipeError):
        pass
